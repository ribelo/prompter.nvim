---@diagnostic disable-next-line: no-unknown
local anthropic = require("prompter_nvim.anthropic.api")
local template = require("prompter_nvim.template")

---@enum endpoints
local ENDPOINTS = {
  "messages",
}

---@enum roles
local ROLES = {
  "user",
  "assistant",
}

---@alias message {role: roles, content: string}
---@alias on_result fun(err: string, output: string)

---@class AnthropicChatRequest
---@field model string
---@field messages message[]
---@field system string
---@field max_tokens integer
---@field metadata table
---@field stop_sequences string[]
---@field stream boolean
---@field temperature number
---@field top_p number
---@field top_k integer
local AnthropicChatRequest = {}
AnthropicChatRequest.__index = AnthropicChatRequest

function AnthropicChatRequest:new(o)
  return setmetatable(o, self)
end

---@class AnthropicCompletionsResponse
---@field id string Unique object identifier.
---@field type string Object type. For Messages, this is always "message".
---@field role string Conversational role of the generated message. This will always be "assistant".
---@field content table[] Content generated by the model. This is an array of content blocks, each of which has a type that determines its shape. Currently, the only type in responses is "text".
---@field model string The model that handled the request.
---@field stop_reason string The reason that we stopped. This may be one of the following values: "end_turn", "max_tokens", "stop_sequence".
---@field stop_sequence string|nil Which custom stop sequence was generated, if any. This value will be a non-null string if one of your custom stop sequences was generated.
---@field usage AnthropicCompletionsUsage Billing and rate-limit usage.

---@class AnthropicCompletionsUsage
---@field input_tokens integer The number of input tokens which were used.
---@field output_tokens integer The number of output tokens which were used.

---@param on_result fun(err: string, response: AnthropicCompletionsResponse)
function AnthropicChatRequest:send(on_result)
  local body = {
    model = self.model,
    messages = self.messages,
  }
  if self.system then
    body.system = self.system
  end
  if self.max_tokens then
    body.max_tokens = self.max_tokens
  end
  if self.metadata then
    body.metadata = self.metadata
  end
  if self.stop_sequences then
    body.stop_sequences = self.stop_sequences
  end
  if self.stream ~= nil then
    body.stream = self.stream
  end
  if self.temperature then
    body.temperature = self.temperature
  end
  if self.top_p then
    body.top_p = self.top_p
  end
  if self.top_k then
    body.top_k = self.top_k
  end
  anthropic.call("messages", body, on_result)
end

---@param params table?
function AnthropicChatRequest:fill(params)
  if self.messages == nil then
    return
  end

  if self.system then
    self.system = template.fill_template(self.system, params)
  end
  for _, message in ipairs(self.messages) do
    message.content = template.fill_template(message.content, params)
  end
end

return AnthropicChatRequest
